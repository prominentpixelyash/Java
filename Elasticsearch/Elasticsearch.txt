# Elasticsearch

- It is a search engine which is based on Apache Lucene.
- Elasticsearch can be used **to search any kind of document**.
- It is an analytics engine for all types of data, including textual, numerical, geospatial, structured, and unstructured.
- It was my first release in 2010 by Elasticsearch N.V.
- Known for its simple REST APIs, distributed nature, speed, and scalability.
- It is using document-oriented approach instead of the Table schema.
- It is so quick as soon as the data is imported into elastic search.

### Node:

- A node is a single server.
- It is a part of the cluster, storing data and participating in the cluster’s indexing and search capability.

### Cluster:

- A Cluster is a collection of one or more nodes that together hold entire data.
- It provides federated indexing and search capability across all nodes and It has a unique name.
- We can also change its name by changing the configuration files.

### Index:

- It is a collection of documents with similar data.
- It can be identified by name.
- the name of indexing is used to index, search, update, and delete documents.

### Type:

- It is a partition of the index.
- We can define one or more types inside the index.

### Document:

- It is a basic unit of information that can be indexed.
- It is in the form of JSON.

### Shards:

- Elastic search provides subdivide index into multiple pieces known as shards.

### Replica:

- Elastic search provides you to make one or more copies of index shards which is known as replica shards or replica.

## Elasticsearch APIs:

1. Document APIs
2. Search APIs
3. Aggregation 
4. Index APIs
5. Cluster APIs

## Document API:

### Index:

Example: 

```json
# to create a default parameter index
PUT students
```

### Get:

Example:

```json
GET _cluster/health
GET _cat/nodes?v
GET _cat/shards?v

# to get doc with id
GET students/_doc/wsoDv4gBC8Nnvp7MHe6a

# to get all docs
GET students/_search
{
  "query": {
    "match_all": {}
  }
}
```

### Update:

```json
# to create own parameter indedx
PUT students
{
  "settings": {
    "number_of_shards": 2,
    "number_of_replicas": 2
  }
}

# to post the doc in index 
POST students/_doc
{
  "name":"Yati",
  "address":"Surat"
}

# to post the doc with our _id
POST students/_doc/123456789
{
  "name":"Nitin",
  "address":"Mumbai"
}

# to replace the doc
PUT students/_doc/123456789
{
  "address":"Pune",
  "post":"spring boot developer"
}

# to update the student 
POST students/_update/123456789
{
  "doc": {

    "address":"Pune",
    "curr_add":"Surat"
    
  }
}

# update the doc with doc
POST students/_update/xcoPv4gBC8Nnvp7Mnu57
{
  
  "doc": {
    
    "age":23
    
  }
  
}

# to replace the doc with post
POST students/_doc/123456789
{
  "name":"Nitin",
  "address":"Mumbai"
}
```

### Delete:

- Example:

```json
# to delete index
DELETE students

# to delete doc
DELETE students/_doc/123456789
```

### Script in docs:

```json
# script in docs
POST students/_update/xcoPv4gBC8Nnvp7Mnu57
{
  
  "script": {
    
    "source": "ctx._source.age=24"
    
  }
  
}

POST students/_update/xcoPv4gBC8Nnvp7Mnu57
{
  "script": {
    
    "source": "ctx._source.age=params.new_age",
    "params": {
      "new_age":5
    }
    
  }
}

POST students/_update/xcoPv4gBC8Nnvp7Mnu57
{
  "script": {
    "source": """
    
    if(ctx._source.age<=0){
      ctx.op='noop'
    }
    else{
     ctx._source.age--; 
    }"""
  }
}

POST students/_update/xcoPv4gBC8Nnvp7Mnu57
{
  "script": {
    "source": """
    
    if(ctx._source.age<=0){
      ctx.op='delete'
    }
    else{
     ctx._source.age--; 
    }"""
  }
}

POST students/_update/xcoPv4gBC8Nnvp7Mnu57
{
  "script": {
    "source": """
    
    if(ctx._source.age<=0){
      ctx.op='delete'
    }
    else{
     ctx._source.age--; 
    }"""
  },
  
  "upsert": {
    
      "name":"Yati",
      "address":"Surat"
    
  }
  
}
```

### Query:

- A query is made up of two clauses:
1. **Leaf Query Clauses:** These clauses are match, term, or range, which look for a specific value in a specific field.
2. **Compound Query Clauses:** These queries are a combination of leaf query clauses and other compound queries to extract the desired information.

- Following types of query
1. Full-Text Queries.
2. Multi Match Query.
3. Query String Query.
4. Term Level Queries.
5. Range Query. 
6. Compound Queries.

- Use the below CSV file

[Amazon_Prime_TV_Shows_Data.csv](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/cc43d866-9a1d-4482-b944-76608e73f0d3/Amazon_Prime_TV_Shows_Data.csv)

- ****Match All Query:****
- It will return all content from the document.

```json

//By default we get only 10 result display
POST tvshows/_search
{
  "query": {
    "match_all": {}
  }
}

//It will display 100 results
GET tvshows/_search
{
  "size": 100, 
  "query": {
    "match_all": {}
  }
}
```

- It will display only 10 records but it will find all records 503 because the size in search is by default 10 if we change the size in search also if we want we need to define size 25.

```json
POST tvshows/_search
{
  "size": 25, 
  "query": {
    "match_all": {}
  }
}
```

1. Full-Text Queries:
- Match:

```json
#direct search with match query
GET tvshows/_search
{
  "size": 25, 
  "query": {
    "match": {
      "Name of the show": "man"
    }
  }
}

# match with and operator 
GET tvshows/_search
{
  "query": {
    "match": {
      "Name of the show": {
        "query":"the terror",
        "operator":"and"
      }
    }
  }
}

#search with field parameter match query
GET tvshows/_search
{
  "query": {
    "match": {
      "Name of the show": {
        "query":"th terr",
        "operator":"and",
        "fuzziness":2
      }
    }
  }
}

#direct search with a match query
GET tvshows/_search
{
  "size": 25, 
  "query": {
    "match": {
      "Name of the show": "man"
    }
  }
}

GET tvshows/_search
{
  "query": {
    "match_phrase": {
      "Name of the show": "family Man"
    }
  }
}
```

1. **Multi Match Query:**

Example:

```json
// multi_match query 
GET tvshows/_search
{
  "query": {
    "multi_match": {
      "query": "Action",
      "fields": ["Name of the show","Genre"],
      "operator": "OR",
      "type": "cross_fields"
    }
  }
}
```

1. **Query String Query:**

```json
GET tvshows/_search
{
  "query": {
    "query_string": {
      "default_field": "Name of the show",
      "query": "the man",
      "default_operator": "AND"
    }
  }
}
```

1. **Term Level Queries:**

```json
POST person/_search
{
  "size": 0, 
  "aggs": {
    "countryCount": {
      "terms": {
        "field": "country"
      }
    }
  }
}

POST person/_search
{
  "size": 0, 
  "aggs": {
    "countrywithsatte": {
      "terms": {
        "field": "country"
      },
      "aggs": {
        "stateCount": {
          "terms": {
            "field": "state"
          }
        }
      }
    }
  }
}

```

1. **Range Query:**
- Example:

```json
GET tvshows/_search
{
  "query": {
    "range": {
      "IMDb rating": {
        "gte": 8.5,
        "lte": 10
      }
    }
  }
}
```

1. **Compound Queries:**
- Example:

```json
GET tvshows/_search
{
  "query": {
    "bool": {
      "must": [
        {"match": {
          "Name of the show": "The"
        }},
        {
          "match": {
            "Language": "Hindi"
          }
        }
      ]
    }
  }
}

GET tvshows/_search
{
  "query": {
    "bool": {
      "should": [
        {
          "match": {
            "Name of the show": "Man"
          }
          
        },
        {
          "match": {
            "Name of the show": "Terror"
          }
        }
      ]
    }
  }
}

GET tvshows/_search
{
  "query": {
    "bool": {
      "must_not": [
        {
          "match": {
            "Name of the show": "THE"
          }
        }
      ]
    }
  }
}

GET tvshows/_search
{
  "query": {
    "bool": {
      "must": [
        {
          "match": {
            "Name of the show": "Man"
          }
        },
        {"match": {
          "Language": "English"
        }
        }
      ],
      "should": [
        {
          "match": {
            "IMDb rating": "4.5"
          }
        }
      ],
      "must_not": [
        {"match": {
          "Name of the show": "Adventures"
        }}
      ]
      
    }
  }
}
```

# Highlighter:

- Highlighters enable you to get highlighted snippets from one or more fields in your search results so you can show users where the query matches are.
- Elasticsearch supports three highlighters:
1. `unified`
2. `plain` 
3. `fvh` (fast vector highlighter).

Example:

```json
GET tvshows/_search
{
  "query": {
    "match": {
      "Name of the show": "Man"
    }
  },
  "highlight": {
    "number_of_fragments": 1, 
    "fields": {
      "Name of the show": {}
    }
  }
}
```

# Aggregation:

- Example:

```json

#avg aggregation 
POST tvshows/_search
{
  "aggs": {
    "AvgRating": {
      "avg": {
        "field": "IMDb rating"
      }
    }
  }
}

POST tvshows/_search
{
  "aggs": {
    "minSeasons": {
      "min": {
        "field": "No of seasons available"
      }
    }
  }
}

POST tvshows/_search
{
  "aggs": {
    "maxSeasons": {
      "max": {
        "field": "No of seasons available"
      }
    }
  }
}

POST tvshows/_search
{
  "aggs": {
    "sumOfSerialNumer": {
      "sum": {
        "field": "S_no_"
      }
    }
  }
}

POST tvshows/_search
{
  "aggs": {
    "totalStats": {
      "stats": {
        "field": "No of seasons available"
      }
    }
  }
}

POST tvshows/_search
{
  "aggs": {
    "distinctRatingCount": {
      "cardinality": {
        "field": "S_no_"
      }
    }
  }
}

POST tvshows/_search
{
  "size": 0, 
  "aggs": {
    "distinctSeasonsCount": {
      "cardinality": {
        "field": "No of seasons available"
      }
    }
  }
}

```

CSV: 

[Amazon_Prime_TV_Shows_Data.csv](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/0c403fb8-cc11-4f38-a508-26a15b683cd1/Amazon_Prime_TV_Shows_Data.csv)

## Field Data type:

- some commonly used field data type in Elasticsearch is as follow.
1. **Text**: Used for full-text search and indexing of textual data. Supports tokenization, stemming, and other text analysis techniques.
2. **Keyword**: Used for exact matching and filtering. Suitable for fields with keywords, IDs, or other values that should not be tokenized.
3. **Date**: Used for storing and indexing date and time values. Supports date-related operations like range queries and sorting.
4. **Numeric**: Includes various numeric data types, such as **`integer`**, **`long`**, **`float`**, and **`double`**. Allows for efficient indexing and querying of numeric values.
5. **Boolean**: Used to represent Boolean values (**`true`** or **`false`**).
6. **Object**: Used to define complex nested structures. Allows for hierarchical organization of data within a document.
7. **Array**: Used to store arrays or lists of values. Enables indexing and searching within arrays.
8. **Geo-point**: Used to represent geographical coordinates (latitude and longitude). Enables spatial queries like distance calculations and bounding box searches.
9. **Geo-shape**: Used to represent more complex geometric shapes, such as polygons or circles. Allows for spatial indexing and querying.

- Example:

```json
# creating index with field data type
PUT customers
{
  "mappings": {
    "properties": {
      "dob":{
        "type": "date"
      },
      "intro":{
        "type": "text"
      },
      "name":{
        "type": "text"
      },
      "age":{
        "type": "integer"
      },
      "marriageStatus":{
        "type": "boolean"
      }
    }
  }
}

# view of index
{
  "customers" : {
    "aliases" : { },
    "mappings" : {
      "properties" : {
        "age" : {
          "type" : "integer"
        },
        "dob" : {
          "type" : "date"
        },
        "intro" : {
          "type" : "text"
        },
        "marriageStatus" : {
          "type" : "boolean"
        },
        "name" : {
          "type" : "text"
        }
      }
    },
    "settings" : {
      "index" : {
        "routing" : {
          "allocation" : {
            "include" : {
              "_tier_preference" : "data_content"
            }
          }
        },
        "number_of_shards" : "1",
        "provided_name" : "customers",
        "creation_date" : "1687430405500",
        "number_of_replicas" : "1",
        "uuid" : "rWEVrD1jTs6oeew3o77S-w",
        "version" : {
          "created" : "7100299"
        }
      }
    }
  }
}

# Example of data type 
POST customers/_doc
{
  "name":"Yash",
  "dob":"1993-08-26",
  "age":29,
  "marriageStatus":false,
  "intro":"Hello I am yash rajput from surat gujarat. I have completed my graduation in Electrical Engineering in 2016 and I have 4+ year of experiance in electrical after quit my job to learn programming language and I learn Html,Css,JavaScript, core java, Advance java, JDBC, Hibernate, Spring, SpringBoot and MySql database"
}

 
```

- Other data are available on [Official Documentation](https://www.elastic.co/guide/en/elasticsearch/reference/current/mapping-types.html)

### Tokenizer:

- some tokenizer are
1. **Standard Tokenizer:** The `standard` tokenizer divides text into terms on word boundaries, as defined by the Unicode Text Segmentation algorithm. It removes most punctuation symbols. It is the best choice for most languages.
- Example:

```json
POST _analyze
{
  "tokenizer": "standard",
  "text": "Hello @Yash to ProminentP!xel prominentpixel@gmail.com"
}
```

1. **Letter Tokenizer:** The `letter` tokenizer divides text into terms whenever it encounters a character which is not a letter.
- Example:

```json
POST _analyze
{
  "tokenizer": "letter",
  "text": "Hello @Yash to ProminentP!xel prominentpixel@gmail.com"
}
```

1. **Lowercase Tokenizer:** The `lowercase` tokenizer, like the `letter` tokenizer, divides text into terms whenever it encounters a character which is not a letter, but it also lowercases all terms.
- Example:

```json
POST _analyze
{
  "tokenizer": "lowercase",
  "text": "Hello @Yash to ProminentP!xel prominentpixel@gmail.com"
}
```

1. **Whitespace Tokenizer:** The `whitespace` tokenizer divides text into terms whenever it encounters any whitespace character.
- Example:

```json
POST _analyze
{
  "tokenizer": "whitespace",
  "text": "Hello @Yash to ProminentP!xel prominentpixel@gmail.com"
}
```

1. **UAX URL Email Tokenizer:** The `uax_url_email` tokenizer is like the `standard` tokenizer except that it recognizes URLs and email addresses as single tokens.
- Example:

```json
POST _analyze
{
  "tokenizer": "uax_url_email",
  "text": "Hello @Yash to ProminentP!xel prominentpixel@gmail.com"
}
```

1. **Classic Tokenizer:** The `classic` tokenizer is a grammar based tokenizer for the English Language.
- Example:

```json
POST _analyze
{
  "tokenizer": "classic",
  "text": "Hello @Yash to ProminentP!xel prominentpixel@gmail.com"
}
```

1. **Keyword tokenizer:**  The `keyword` tokenizer is a “noop” tokenizer that accepts whatever text it is given and outputs the exact same text as a single term.
- Example:

```json
POST _analyze
{
  "tokenizer": "keyword",
  "text": "Hello @Yash to ProminentP!xel prominentpixel@gmail.com"
}
```

1. **Pattern Tokenizer:** The `pattern` tokenizer uses a regular expression to either split text into terms whenever it matches a word separator, or to capture matching text as terms.
- Example:

```json
POST _analyze
{
  "tokenizer":{
    "type": "pattern",
    "pattern":","
  },
  "text": "Hell,o @Ya,sh to Pro,minen,tP!xel prom,inentpixel@gmail.com"
}
```

1. **Simple Pattern Tokenizer:** The `simple_pattern` tokenizer uses a regular expression to capture matching text as terms. It uses a restricted subset of regular expression features and is generally faster than the `pattern` tokenizer.
- Example:

```json
POST _analyze
{
  "tokenizer": {
    "type":"char_group",
    "tokenize_on_chars":[
      "whitespace",
      "_"
      ]
  },
  "text": "H_ello @Yash to Pro_minent_P!xel prominent_pixel_@gmail.com"
}
```

1. **Char Group Tokenizer:** he `char_group` tokenizer is configurable through sets of characters to split on, which is usually less expensive than running regular expressions.
- Example:

```json
POST _analyze
{
  "tokenizer": {
    "type":"char_group",
    "tokenize_on_chars":[
      "whitespace",
      "_"
      ]
  },
  "text": "H_ello @Yash to Pro_minent_P!xel prominent_pixel_@gmail.com"
```

1. **Simple Pattern Split Tokenizer:**

The `simple_pattern_split` tokenizer uses the same restricted regular expression subset as the `simple_pattern` tokenizer, but splits the input at matches rather than returning the matches as terms.

- Example:

```json
POST _analyze
{
  "tokenizer": {
    "type": "simple_pattern_split",
    "pattern": "_"
  },
  "text": "Hello @Ya_sh to Prominent_P!xel prominent_pixel_@gmail.com"
}
```

### Filter:

- Following are some type and example:
1. **Apostrophe token filter:**
- Strips all characters after an apostrophe, including the apostrophe itself.
- Example:

```json
GET _analyze
{
  "tokenizer": "standard",
  "filter": ["apostrophe"],
  "text": "You'r did'not know"
}
```

- Output:
    
    ```json
    {
      "tokens" : [
        {
          "token" : "You",
          "start_offset" : 0,
          "end_offset" : 5,
          "type" : "<ALPHANUM>",
          "position" : 0
        },
        {
          "token" : "did",
          "start_offset" : 6,
          "end_offset" : 13,
          "type" : "<ALPHANUM>",
          "position" : 1
        },
        {
          "token" : "know",
          "start_offset" : 14,
          "end_offset" : 18,
          "type" : "<ALPHANUM>",
          "position" : 2
        }
      ]
    }
    ```
    

1. **Classic token filter:**
- This filter removes the English possessive (`'s`) from the end of words and removes dots from acronyms. It uses Lucene’s ClassicFilter.
- Example:

```json
GET _analyze
{
  "tokenizer": "classic",
  "filter": ["classic"],
  "text": "The 2 Q.U.I.C.K. Brown-Foxes jumped over the lazy dog's bone."
}
```

- Output:
    
    ```json
    {
      "tokens" : [
        {
          "token" : "The",
          "start_offset" : 0,
          "end_offset" : 3,
          "type" : "<ALPHANUM>",
          "position" : 0
        },
        {
          "token" : "2",
          "start_offset" : 4,
          "end_offset" : 5,
          "type" : "<ALPHANUM>",
          "position" : 1
        },
        {
          "token" : "QUICK",
          "start_offset" : 6,
          "end_offset" : 16,
          "type" : "<ACRONYM>",
          "position" : 2
        },
        {
          "token" : "Brown",
          "start_offset" : 17,
          "end_offset" : 22,
          "type" : "<ALPHANUM>",
          "position" : 3
        },
        {
          "token" : "Foxes",
          "start_offset" : 23,
          "end_offset" : 28,
          "type" : "<ALPHANUM>",
          "position" : 4
        },
        {
          "token" : "jumped",
          "start_offset" : 29,
          "end_offset" : 35,
          "type" : "<ALPHANUM>",
          "position" : 5
        },
        {
          "token" : "over",
          "start_offset" : 36,
          "end_offset" : 40,
          "type" : "<ALPHANUM>",
          "position" : 6
        },
        {
          "token" : "the",
          "start_offset" : 41,
          "end_offset" : 44,
          "type" : "<ALPHANUM>",
          "position" : 7
        },
        {
          "token" : "lazy",
          "start_offset" : 45,
          "end_offset" : 49,
          "type" : "<ALPHANUM>",
          "position" : 8
        },
        {
          "token" : "dog",
          "start_offset" : 50,
          "end_offset" : 55,
          "type" : "<APOSTROPHE>",
          "position" : 9
        },
        {
          "token" : "bone",
          "start_offset" : 56,
          "end_offset" : 60,
          "type" : "<ALPHANUM>",
          "position" : 10
        }
      ]
    }
    ```
    

1. **Common grams token filter**

Generates bigrams for a specified set of common words.

For example, you can specify `is` and `the` as common words. This filter then converts the tokens `[the, quick, fox, is, brown]` to `[the, the_quick, quick,
fox, fox_is, is, is_brown, brown]`.

- Example:

```json
GET _analyze
{
  "tokenizer": "whitespace",
  "filter": [{
    "type" : "common_grams",
    "common_words":["the","is"]
  }],
  "text" : "the quick fox is brown"
}
```

- Output:
    
    ```json
    {
      "tokens" : [
        {
          "token" : "the",
          "start_offset" : 0,
          "end_offset" : 3,
          "type" : "word",
          "position" : 0
        },
        {
          "token" : "the_quick",
          "start_offset" : 0,
          "end_offset" : 9,
          "type" : "gram",
          "position" : 0,
          "positionLength" : 2
        },
        {
          "token" : "quick",
          "start_offset" : 4,
          "end_offset" : 9,
          "type" : "word",
          "position" : 1
        },
        {
          "token" : "fox",
          "start_offset" : 10,
          "end_offset" : 13,
          "type" : "word",
          "position" : 2
        },
        {
          "token" : "fox_is",
          "start_offset" : 10,
          "end_offset" : 16,
          "type" : "gram",
          "position" : 2,
          "positionLength" : 2
        },
        {
          "token" : "is",
          "start_offset" : 14,
          "end_offset" : 16,
          "type" : "word",
          "position" : 3
        },
        {
          "token" : "is_brown",
          "start_offset" : 14,
          "end_offset" : 22,
          "type" : "gram",
          "position" : 3,
          "positionLength" : 2
        },
        {
          "token" : "brown",
          "start_offset" : 17,
          "end_offset" : 22,
          "type" : "word",
          "position" : 4
        }
      ]
    }
    ```
    

1. **Conditional token filter:**
- Applies a set of token filters to tokens that match conditions in a provided predicate script.
- Example:

```json
GET _analyze
{
  "tokenizer": "standard",
  "filter":[{
    "type":"condition",
    "filter": "lowercase",
    "script":{
      "source": "token.getTerm().length() < 5"
    }
  }],
  "text": "THE QUICK BROWN FOX"
}
```

- Output:
    
    ```json
    {
      "tokens" : [
        {
          "token" : "the",
          "start_offset" : 0,
          "end_offset" : 3,
          "type" : "<ALPHANUM>",
          "position" : 0
        },
        {
          "token" : "QUICK",
          "start_offset" : 4,
          "end_offset" : 9,
          "type" : "<ALPHANUM>",
          "position" : 1
        },
        {
          "token" : "BROWN",
          "start_offset" : 10,
          "end_offset" : 15,
          "type" : "<ALPHANUM>",
          "position" : 2
        },
        {
          "token" : "fox",
          "start_offset" : 16,
          "end_offset" : 19,
          "type" : "<ALPHANUM>",
          "position" : 3
        }
      ]
    }
    ```
    

1. **Decimal digit token filter:**

Converts all digits in the Unicode `Decimal_Number` General Category to `0-9`. For example, the filter changes the Bengali numeral `৩` to `3`.

- Example:

```json
GET _analyze
{
  "tokenizer": "whitespace",
  "filter": ["decimal_digit"],
  "text": "१-one two-२ ३"
}
```

- Output:
    
    ```json
    {
      "tokens" : [
        {
          "token" : "1-one",
          "start_offset" : 0,
          "end_offset" : 5,
          "type" : "word",
          "position" : 0
        },
        {
          "token" : "two-2",
          "start_offset" : 6,
          "end_offset" : 11,
          "type" : "word",
          "position" : 1
        },
        {
          "token" : "3",
          "start_offset" : 12,
          "end_offset" : 13,
          "type" : "word",
          "position" : 2
        }
      ]
    }
    ```
    

1. **Delimited payload token filter:**  Separates a token stream into tokens and payloads based on a specified delimiter.
- Example: you can use the `delimited_payload` filter with a `|` delimiter to split `the|1 quick|2 fox|3` into the tokens `the`, `quick`, and `fox` with respective payloads of `1`, `2`, and `3`.

```json
GET _analyze 
{
  "tokenizer": "whitespace"
  , "filter": ["delimited_payload"]
  , "text": "Special Message|20 Hello|21 Java|1 welcome |6 to|5 ProminentPixel"
}
```

- Output:
    
    ```json
    {
      "tokens" : [
        {
          "token" : "Special",
          "start_offset" : 0,
          "end_offset" : 7,
          "type" : "word",
          "position" : 0
        },
        {
          "token" : "Message",
          "start_offset" : 8,
          "end_offset" : 18,
          "type" : "word",
          "position" : 1
        },
        {
          "token" : "Hello",
          "start_offset" : 19,
          "end_offset" : 27,
          "type" : "word",
          "position" : 2
        },
        {
          "token" : "Java",
          "start_offset" : 28,
          "end_offset" : 34,
          "type" : "word",
          "position" : 3
        },
        {
          "token" : "welcome",
          "start_offset" : 35,
          "end_offset" : 42,
          "type" : "word",
          "position" : 4
        },
        {
          "token" : "",
          "start_offset" : 43,
          "end_offset" : 45,
          "type" : "word",
          "position" : 5
        },
        {
          "token" : "to",
          "start_offset" : 46,
          "end_offset" : 50,
          "type" : "word",
          "position" : 6
        },
        {
          "token" : "ProminentPixel",
          "start_offset" : 51,
          "end_offset" : 65,
          "type" : "word",
          "position" : 7
        }
      ]
    }
    ```
    

1. **Dictionary decompounder token filter:**  Uses a specified list of words and a brute force approach to find sub words in compound words. If found, these sub words are included in the token output.

- Example: The following [analyze API](https://www.elastic.co/guide/en/elasticsearch/reference/current/indices-analyze.html) request uses the `dictionary_decompounder` filter to find sub words in **HelloJavaWelcometoProminentPixel**. The filter then checks these sub words against the specified list of words: **Java** and **ProminentPixel.**

```json
GET _analyze
{
  "tokenizer": "standard",
  "filter": [
    {
      "type": "dictionary_decompounder",
      "word_list":["Java","ProminentPixel"]
    }],
    "text": "HelloJavaWelcometoProminentPixel"
}
```

- Output:
    
    ```json
    {
      "tokens" : [
        {
          "token" : "HelloJavaWelcometoProminentPixel",
          "start_offset" : 0,
          "end_offset" : 32,
          "type" : "<ALPHANUM>",
          "position" : 0
        },
        {
          "token" : "Java",
          "start_offset" : 0,
          "end_offset" : 32,
          "type" : "<ALPHANUM>",
          "position" : 0
        },
        {
          "token" : "ProminentPixel",
          "start_offset" : 0,
          "end_offset" : 32,
          "type" : "<ALPHANUM>",
          "position" : 0
        }
      ]
    } 
    ```
    

1. **Edge n-gram token filter:**
- Forms an [n-gram](https://en.wikipedia.org/wiki/N-gram) of a specified length from the beginning of a token.
- For example, you can use the `edge_ngram` token filter to change `quick` to `qu`.
- When not customized, the filter creates 1-character edge n-grams by default.
- Example: The following [analyze API](https://www.elastic.co/guide/en/elasticsearch/reference/current/indices-analyze.html) request uses the `edge_ngram` filter to convert **Hello Java Welcome to ProminentPixel** to 1-character and 2-character edge n-grams:

```json
GET _analyze
{
  "tokenizer": "standard",
  "filter": [{
    "type" : "edge_ngram",
    "min_gram" : 1,
    "max_gram" : 2
  }],
  "text": "Hello Java Welcome to ProminentPixel"
}
```

- Output:
    
    ```json
    {
      "tokens" : [
        {
          "token" : "H",
          "start_offset" : 0,
          "end_offset" : 5,
          "type" : "<ALPHANUM>",
          "position" : 0
        },
        {
          "token" : "He",
          "start_offset" : 0,
          "end_offset" : 5,
          "type" : "<ALPHANUM>",
          "position" : 0
        },
        {
          "token" : "J",
          "start_offset" : 6,
          "end_offset" : 10,
          "type" : "<ALPHANUM>",
          "position" : 1
        },
        {
          "token" : "Ja",
          "start_offset" : 6,
          "end_offset" : 10,
          "type" : "<ALPHANUM>",
          "position" : 1
        },
        {
          "token" : "W",
          "start_offset" : 11,
          "end_offset" : 18,
          "type" : "<ALPHANUM>",
          "position" : 2
        },
        {
          "token" : "We",
          "start_offset" : 11,
          "end_offset" : 18,
          "type" : "<ALPHANUM>",
          "position" : 2
        },
        {
          "token" : "t",
          "start_offset" : 19,
          "end_offset" : 21,
          "type" : "<ALPHANUM>",
          "position" : 3
        },
        {
          "token" : "to",
          "start_offset" : 19,
          "end_offset" : 21,
          "type" : "<ALPHANUM>",
          "position" : 3
        },
        {
          "token" : "P",
          "start_offset" : 22,
          "end_offset" : 36,
          "type" : "<ALPHANUM>",
          "position" : 4
        },
        {
          "token" : "Pr",
          "start_offset" : 22,
          "end_offset" : 36,
          "type" : "<ALPHANUM>",
          "position" : 4
        }
      ]
    }
    ```
    

1. **Elision token filter:** Removes specified [elisions](https://en.wikipedia.org/wiki/Elision) from the beginning of tokens. For example, you can use this filter to change `l'avion` to `avion`.
- When not customized, the filter removes the following French elisions by default:

        `l'`, `m'`, `t'`, `qu'`, `n'`, `s'`, `j'`, `d'`, `c'`, `jusqu'`, `quoiqu'`, `lorsqu'`, `puisqu'`

- Example:

```json
GET _analyze
{
  "tokenizer" : "standard",
  "filter" : ["elision"],
  "text" : "Hello J'ava c'ome to n'max"
}
```

- Output
    
    ```json
    {
      "tokens" : [
        {
          "token" : "Hello",
          "start_offset" : 0,
          "end_offset" : 5,
          "type" : "<ALPHANUM>",
          "position" : 0
        },
        {
          "token" : "ava",
          "start_offset" : 6,
          "end_offset" : 11,
          "type" : "<ALPHANUM>",
          "position" : 1
        },
        {
          "token" : "ome",
          "start_offset" : 12,
          "end_offset" : 17,
          "type" : "<ALPHANUM>",
          "position" : 2
        },
        {
          "token" : "to",
          "start_offset" : 18,
          "end_offset" : 20,
          "type" : "<ALPHANUM>",
          "position" : 3
        },
        {
          "token" : "max",
          "start_offset" : 21,
          "end_offset" : 26,
          "type" : "<ALPHANUM>",
          "position" : 4
        }
      ]
    }
    ```
    

1. **Fingerprint token filter:**
- Sorts and removes duplicate tokens from a token stream, then concatenates the stream into a single output token.
- For example, this filter changes the `[ the, fox, was, very, very, quick ]` token stream as follows:
1. Sorts the tokens alphabetically to `[ fox, quick, the, very, very, was ]`
2. Removes a duplicate instance of the `very` token.
3. Concatenates the token stream to a output single token: `[fox quick the very was ]`

- Example:

```json
GET _analyze
{
  "tokenizer": "standard",
  "filter": ["fingerprint"], 
  "text": "Java Hello Welcome Welcome to World"
}
```

- Output:
    
    ```json
    {
      "tokens" : [
        {
          "token" : "Hello Java Welcome World to",
          "start_offset" : 0,
          "end_offset" : 35,
          "type" : "fingerprint",
          "position" : 0
        }
      ]
    }
    ```
    
    1. 

1. **Flatten graph token filter:** Flattens a [token graph](https://www.elastic.co/guide/en/elasticsearch/reference/current/token-graphs.html) produced by a graph tokens filter, such as `[synonym_graph](https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-synonym-graph-tokenfilter.html)` or `[word_delimiter_graph](https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-word-delimiter-graph-tokenfilter.html)`.

- Example:

```json
GET _analyze
{
  "tokenizer": "standard",
  "filter": [{
    "type":"synonym_graph",
    "synonyms":["jdbc,Java Database Connectivity"]
  }],
  "text": "Java Database Connectivity is the base of all database connectivity"
}
```

- Output:
    
    ```json
    {
      "tokens" : [
        {
          "token" : "jdbc",
          "start_offset" : 0,
          "end_offset" : 26,
          "type" : "SYNONYM",
          "position" : 0,
          "positionLength" : 3
        },
        {
          "token" : "Java",
          "start_offset" : 0,
          "end_offset" : 4,
          "type" : "<ALPHANUM>",
          "position" : 0
        },
        {
          "token" : "Database",
          "start_offset" : 5,
          "end_offset" : 13,
          "type" : "<ALPHANUM>",
          "position" : 1
        },
        {
          "token" : "Connectivity",
          "start_offset" : 14,
          "end_offset" : 26,
          "type" : "<ALPHANUM>",
          "position" : 2
        },
        {
          "token" : "is",
          "start_offset" : 27,
          "end_offset" : 29,
          "type" : "<ALPHANUM>",
          "position" : 3
        },
        {
          "token" : "the",
          "start_offset" : 30,
          "end_offset" : 33,
          "type" : "<ALPHANUM>",
          "position" : 4
        },
        {
          "token" : "base",
          "start_offset" : 34,
          "end_offset" : 38,
          "type" : "<ALPHANUM>",
          "position" : 5
        },
        {
          "token" : "of",
          "start_offset" : 39,
          "end_offset" : 41,
          "type" : "<ALPHANUM>",
          "position" : 6
        },
        {
          "token" : "all",
          "start_offset" : 42,
          "end_offset" : 45,
          "type" : "<ALPHANUM>",
          "position" : 7
        },
        {
          "token" : "database",
          "start_offset" : 46,
          "end_offset" : 54,
          "type" : "<ALPHANUM>",
          "position" : 8
        },
        {
          "token" : "connectivity",
          "start_offset" : 55,
          "end_offset" : 67,
          "type" : "<ALPHANUM>",
          "position" : 9
        }
      ]
    }
    ```
    

1. **Unique token filter:** 

Removes duplicate tokens from a stream. For example, you can use the `unique` filter to change `the lazy lazy dog` to `the lazy dog`.

- Example:

```json
GET _analyze
{
  "tokenizer" : "whitespace",
  "filter" : ["unique"],
  "text" : "the quick fox jumps the lazy fox"
}
```

- Output:
    
    ```json
    {
      "tokens" : [
        {
          "token" : "the",
          "start_offset" : 0,
          "end_offset" : 3,
          "type" : "word",
          "position" : 0
        },
        {
          "token" : "quick",
          "start_offset" : 4,
          "end_offset" : 9,
          "type" : "word",
          "position" : 1
        },
        {
          "token" : "fox",
          "start_offset" : 10,
          "end_offset" : 13,
          "type" : "word",
          "position" : 2
        },
        {
          "token" : "jumps",
          "start_offset" : 14,
          "end_offset" : 19,
          "type" : "word",
          "position" : 3
        },
        {
          "token" : "lazy",
          "start_offset" : 24,
          "end_offset" : 28,
          "type" : "word",
          "position" : 4
        }
      ]
    }
    ```
    
    1. **Keep types token filter:**
    
    Keeps or removes tokens of a specific type. For example, you can use this filter to change `3 quick foxes` to `quick foxes` by keeping only `<ALPHANUM>` (alphanumeric) tokens.
    

- Example:

```json
GET _analyze
{
  "tokenizer": "standard"
  , "filter": [{
    "type": "keep_types",
    "types":"<NUM>"
  }]
  , "text": "1 java 2 Hello 3 world 4 print 5 it"
}

GET _analyze
{
  "tokenizer": "standard"
  , "filter": [{
    "type": "keep_types",
    "types":["<NUM>"],
    "mode":"exclude"
  }]
  , "text": "1 java 2 Hello 3 world 4 print 5 it"
}
```

- Output:
    
    ```json
    **# Output-1** 
    {
      "tokens" : [
        {
          "token" : "1",
          "start_offset" : 0,
          "end_offset" : 1,
          "type" : "<NUM>",
          "position" : 0
        },
        {
          "token" : "2",
          "start_offset" : 7,
          "end_offset" : 8,
          "type" : "<NUM>",
          "position" : 2
        },
        {
          "token" : "3",
          "start_offset" : 15,
          "end_offset" : 16,
          "type" : "<NUM>",
          "position" : 4
        },
        {
          "token" : "4",
          "start_offset" : 23,
          "end_offset" : 24,
          "type" : "<NUM>",
          "position" : 6
        },
        {
          "token" : "5",
          "start_offset" : 31,
          "end_offset" : 32,
          "type" : "<NUM>",
          "position" : 8
        }
      ]
    }
    
    **# Output-2**
    
    {
      "tokens" : [
        {
          "token" : "java",
          "start_offset" : 2,
          "end_offset" : 6,
          "type" : "<ALPHANUM>",
          "position" : 1
        },
        {
          "token" : "Hello",
          "start_offset" : 9,
          "end_offset" : 14,
          "type" : "<ALPHANUM>",
          "position" : 3
        },
        {
          "token" : "world",
          "start_offset" : 17,
          "end_offset" : 22,
          "type" : "<ALPHANUM>",
          "position" : 5
        },
        {
          "token" : "print",
          "start_offset" : 25,
          "end_offset" : 30,
          "type" : "<ALPHANUM>",
          "position" : 7
        },
        {
          "token" : "it",
          "start_offset" : 33,
          "end_offset" : 35,
          "type" : "<ALPHANUM>",
          "position" : 9
        }
      ]
    }
    ```
    

1. **Keep words token filter:** Keeps only tokens contained in a specified word list.

- Example:

```json
GET _analyze
{
  "tokenizer": "standard",
  "filter": [{
    "type":"keep",
    "keep_words":["Java", "World"]
  }],
  "text": "Welcome to Java World"
}
```

- Output:
    
    ```json
    {
      "tokens" : [
        {
          "token" : "Java",
          "start_offset" : 11,
          "end_offset" : 15,
          "type" : "<ALPHANUM>",
          "position" : 2
        },
        {
          "token" : "World",
          "start_offset" : 16,
          "end_offset" : 21,
          "type" : "<ALPHANUM>",
          "position" : 3
        }
      ]
    }
    ```
    

1. **Keyword marker token filter:** Marks specified tokens as keywords, which are not stemmed.

- Example:

```json
GET _analyze
{
  "tokenizer": "whitespace",
  "filter": ["stemmer"]
  , "text":"Peoples are running in the marathon"
}
```

- Output:
    
    ```json
    {
      "tokens" : [
        {
          "token" : "Peopl",
          "start_offset" : 0,
          "end_offset" : 7,
          "type" : "word",
          "position" : 0
        },
        {
          "token" : "ar",
          "start_offset" : 8,
          "end_offset" : 11,
          "type" : "word",
          "position" : 1
        },
        {
          "token" : "run",
          "start_offset" : 12,
          "end_offset" : 19,
          "type" : "word",
          "position" : 2
        },
        {
          "token" : "in",
          "start_offset" : 20,
          "end_offset" : 22,
          "type" : "word",
          "position" : 3
        },
        {
          "token" : "the",
          "start_offset" : 23,
          "end_offset" : 26,
          "type" : "word",
          "position" : 4
        },
        {
          "token" : "marathon",
          "start_offset" : 27,
          "end_offset" : 35,
          "type" : "word",
          "position" : 5
        }
      ]
    }
    ```
    

1. **Keyword repeat token filter:** Outputs a keyword version of each token in a stream. These keyword tokens are not stemmed.

- Example:

```json
GET _analyze
{
  "tokenizer": "whitespace",
  "filter": ["keyword_repeat"],
  "text": "Hello Java"

```

- Output:
    
    ```json
    {
      "tokens" : [
        {
          "token" : "Hello",
          "start_offset" : 0,
          "end_offset" : 5,
          "type" : "word",
          "position" : 0
        },
        {
          "token" : "Hello",
          "start_offset" : 0,
          "end_offset" : 5,
          "type" : "word",
          "position" : 0
        },
        {
          "token" : "Java",
          "start_offset" : 6,
          "end_offset" : 10,
          "type" : "word",
          "position" : 1
        },
        {
          "token" : "Java",
          "start_offset" : 6,
          "end_offset" : 10,
          "type" : "word",
          "position" : 1
        }
      ]
    }
    ```
    

1. **KStem token filter:** Provides [KStem](https://ciir.cs.umass.edu/pubfiles/ir-35.pdf)-based stemming for the English language. The `kstem` filter combines [algorithmic stemming](https://www.elastic.co/guide/en/elasticsearch/reference/current/stemming.html#algorithmic-stemmers) with a built-in [dictionary](https://www.elastic.co/guide/en/elasticsearch/reference/current/stemming.html#dictionary-stemmers).

- Example:

```json
GET _analyze
{
  "tokenizer": "whitespace"
  , "filter": ["kstem"],
  "text": "the foxes jumping quickly"
}
```

- Output:
    
    ```json
    {
      "tokens" : [
        {
          "token" : "the",
          "start_offset" : 0,
          "end_offset" : 3,
          "type" : "word",
          "position" : 0
        },
        {
          "token" : "fox",
          "start_offset" : 4,
          "end_offset" : 9,
          "type" : "word",
          "position" : 1
        },
        {
          "token" : "jump",
          "start_offset" : 10,
          "end_offset" : 17,
          "type" : "word",
          "position" : 2
        },
        {
          "token" : "quick",
          "start_offset" : 18,
          "end_offset" : 25,
          "type" : "word",
          "position" : 3
        }
      ]
    }
    ```
    

1. **Length token filter:** Removes tokens shorter or longer than specified character lengths. For example, you can use the `length` filter to exclude tokens shorter than 2 characters and tokens longer than 5 characters.

- Example:

```json
GET _analyze
{
  "tokenizer": "whitespace",
  "filter": [{
    "type":"length",
    "min":0,
    "max":4
  }],
  "text": "Yati make nodejs example to explain Yash"
}
```

- Output:
    
    ```json
    {
      "tokens" : [
        {
          "token" : "Yati",
          "start_offset" : 0,
          "end_offset" : 4,
          "type" : "word",
          "position" : 0
        },
        {
          "token" : "make",
          "start_offset" : 5,
          "end_offset" : 9,
          "type" : "word",
          "position" : 1
        },
        {
          "token" : "to",
          "start_offset" : 25,
          "end_offset" : 27,
          "type" : "word",
          "position" : 4
        },
        {
          "token" : "Yash",
          "start_offset" : 36,
          "end_offset" : 40,
          "type" : "word",
          "position" : 6
        }
      ]
    }
    ```
    

1. **Trim token filter:** Removes leading and trailing whitespace from each token in a stream. While this can change the length of a token, the `trim` filter does *not* change a token’s offsets.

- Example:

```json
GET _analyze
{
  "tokenizer": "keyword",
  "text": "   Hello Java    "
}

GET _analyze
{
  "tokenizer": "keyword"
  , "filter": ["trim"],
  "text":"   Hello Java    "
}
```

- Output:
    
    ```json
    # GET _analyze
    {
      "tokens" : [
        {
          "token" : "   Hello Java    ",
          "start_offset" : 0,
          "end_offset" : 17,
          "type" : "word",
          "position" : 0
        }
      ]
    }
    
    # GET _analyze
    {
      "tokens" : [
        {
          "token" : "Hello Java",
          "start_offset" : 0,
          "end_offset" : 17,
          "type" : "word",
          "position" : 0
        }
      ]
    }
    ```
    

1. **Uppercase token filter:** Changes token text to uppercase. For example, you can use the `uppercase` filter to change `the Lazy DoG` to `THE LAZY DOG`.

- Example:

```json
GET _analyze
{
  "tokenizer": "whitespace"
  , "filter": ["uppercase"],
  "text": "hello world WelCome to JAva"
}
```

- Output:
    
    ```json
    {
      "tokens" : [
        {
          "token" : "HELLO",
          "start_offset" : 0,
          "end_offset" : 5,
          "type" : "word",
          "position" : 0
        },
        {
          "token" : "WORLD",
          "start_offset" : 6,
          "end_offset" : 11,
          "type" : "word",
          "position" : 1
        },
        {
          "token" : "WELCOME",
          "start_offset" : 12,
          "end_offset" : 19,
          "type" : "word",
          "position" : 2
        },
        {
          "token" : "TO",
          "start_offset" : 20,
          "end_offset" : 22,
          "type" : "word",
          "position" : 3
        },
        {
          "token" : "JAVA",
          "start_offset" : 23,
          "end_offset" : 27,
          "type" : "word",
          "position" : 4
        }
      ]
    }
    ```
    

1. **Synonym token filter:**

The `synonym` token filter allows to easily handle synonyms during the analysis process. Synonyms are configured using a configuration file.

- Example:

```json
PUT school_products
{
  "settings": {
    "analysis": {
      "analyzer": {
        "my_synonyms":{
          "tokenizer":"standard",
          "filter":["lowercase","synonyms"]
        }
      }
      , "filter": {
        "synonyms":{
            "type":"synonym",
          "synonyms_path": "synonyms\\phone_synonyms.txt"
          }
      }
    }
  }
}

GET school_products/_analyze
{
  "analyzer": "my_synonyms",
  "text": "Iphone"
}
```

- Output:
    
    ```json
    {
      "tokens" : [
        {
          "token" : "iphone",
          "start_offset" : 0,
          "end_offset" : 6,
          "type" : "<ALPHANUM>",
          "position" : 0
        },
        {
          "token" : "i",
          "start_offset" : 0,
          "end_offset" : 6,
          "type" : "SYNONYM",
          "position" : 0
        },
        {
          "token" : "ione",
          "start_offset" : 0,
          "end_offset" : 6,
          "type" : "SYNONYM",
          "position" : 0
        },
        {
          "token" : "ipone",
          "start_offset" : 0,
          "end_offset" : 6,
          "type" : "SYNONYM",
          "position" : 0
        },
        {
          "token" : "ifon",
          "start_offset" : 0,
          "end_offset" : 6,
          "type" : "SYNONYM",
          "position" : 0
        },
        {
          "token" : "phone",
          "start_offset" : 0,
          "end_offset" : 6,
          "type" : "SYNONYM",
          "position" : 1
        }
      ]
    }
    ```
    
- Other Filter are available on the [link](https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-tokenfilters.html)

### Character filters reference:

1. **HTML strip character filter:** Strips HTML elements from a text and replaces HTML entities with their decoded value (e.g., replaces `&amp;` with `&`).

- Example:

```json
GET _analyze
{
  "tokenizer": "standard",
  "char_filter": ["html_strip"],
  "text": "<h1><a>Welocme to java with elasticsearch API</a></h1>"
}
```

- Output:
    
    ```json
    {
      "tokens" : [
        {
          "token" : "Welocme",
          "start_offset" : 7,
          "end_offset" : 14,
          "type" : "<ALPHANUM>",
          "position" : 0
        },
        {
          "token" : "to",
          "start_offset" : 15,
          "end_offset" : 17,
          "type" : "<ALPHANUM>",
          "position" : 1
        },
        {
          "token" : "java",
          "start_offset" : 18,
          "end_offset" : 22,
          "type" : "<ALPHANUM>",
          "position" : 2
        },
        {
          "token" : "with",
          "start_offset" : 23,
          "end_offset" : 27,
          "type" : "<ALPHANUM>",
          "position" : 3
        },
        {
          "token" : "elasticsearch",
          "start_offset" : 28,
          "end_offset" : 41,
          "type" : "<ALPHANUM>",
          "position" : 4
        },
        {
          "token" : "API",
          "start_offset" : 42,
          "end_offset" : 49,
          "type" : "<ALPHANUM>",
          "position" : 5
        }
      ]
    }
    ```
    

1. **Mapping character filter:** The `mapping` character filter accepts a map of keys and values. Whenever it encounters a string of characters that is the same as a key, it replaces them with the value associated with that key.

- Example:

```json
GET _analyze
{
  "tokenizer": "standard",
  "char_filter": [{
    "type" : "mapping",
    "mappings":[
      
      "0 => 0",
      "૧ => 1",
      "૨ => 2",
      "૩ => 3",
      "૪ => 4",
      "૫ => 5",
      "૬ => 6",
      "૭ => 7",
      "૮ => 8",
      "૯ => 9"
      
      ]
  }],
  "text": "My mobile number is ૯0૩૩૭૫૭૧0૧"
}
```

- Output:
    
    ```json
    {
      "tokens" : [
        {
          "token" : "My",
          "start_offset" : 0,
          "end_offset" : 2,
          "type" : "<ALPHANUM>",
          "position" : 0
        },
        {
          "token" : "mobile",
          "start_offset" : 3,
          "end_offset" : 9,
          "type" : "<ALPHANUM>",
          "position" : 1
        },
        {
          "token" : "number",
          "start_offset" : 10,
          "end_offset" : 16,
          "type" : "<ALPHANUM>",
          "position" : 2
        },
        {
          "token" : "is",
          "start_offset" : 17,
          "end_offset" : 19,
          "type" : "<ALPHANUM>",
          "position" : 3
        },
        {
          "token" : "9033757101",
          "start_offset" : 20,
          "end_offset" : 30,
          "type" : "<NUM>",
          "position" : 4
        }
      ]
    }
    ```
    

1. **Pattern replace character filter:** The `pattern_replace` character filter uses a regular expression to match characters which should be replaced with the specified replacement string. The replacement string can refer to capture groups in the regular expression.

- Example:

```json
GET _analyze
{
  "tokenizer": "keyword",
  "char_filter": [{
    "type":"pattern_replace",
    "pattern": "(\\d+)-(?=\\d)",
    "replacement" : "$1^"
  }],
  "text": "Ceadit card number is 123-456-789-101"
}
```

- Output:
{
  "tokens" : [
    {
      "token" : "Ceadit card number is 123^456^789^101",
      "start_offset" : 0,
      "end_offset" : 37,
      "type" : "word",
      "position" : 0
    }
  ]
}